{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Stocks Based On Value At Risk\n",
    "Members of Teams\n",
    "1. Abdullah Nasih Jasir (5025211111)\n",
    "2. Mohammad Ahnaf Fauzan (5025211170)\n",
    "3. Al-Ferro Yudisthira Putra (5025211176)\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# Visualitation Style\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "## **PREPROCESSING**\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Gathering**\n",
    "The following code is useful for importing data from CSV files stored in the history folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'saham'\n",
    "all_rec = glob.iglob(path + '/*.csv', recursive=True)\n",
    "count = 0\n",
    "\n",
    "prices_df = pd.DataFrame()\n",
    "for f in all_rec:\n",
    "    count = count + 1\n",
    "    df = pd.read_csv(f, index_col='date', usecols=['date', 'close'])\n",
    "    colname = os.path.basename(f).replace('.csv', '')\n",
    "    df.rename(columns={'close': colname}, inplace=True)\n",
    "    prices_df = pd.concat([prices_df, df], axis=1, sort=False)\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "prices_df.index = pd.to_datetime(prices_df.index)\n",
    "\n",
    "# Define the date range\n",
    "start_date = pd.to_datetime(\"2022-03-24\")\n",
    "end_date = pd.to_datetime(\"2023-03-24\")\n",
    "\n",
    "# Filter the data to include only the specified date range\n",
    "prices_df = prices_df[(prices_df.index >= start_date) & (prices_df.index <= end_date)]\n",
    "\n",
    "# Filter stocks with at least 100 data points within the date range\n",
    "valid_stocks = prices_df.columns[prices_df.count() >= 200]\n",
    "\n",
    "# Create a new DataFrame with the selected date range and valid stocks\n",
    "prices_train = prices_df.loc[(prices_df.index >= start_date) & (prices_df.index <= end_date),valid_stocks]\n",
    "prices_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Assesing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_train.info(any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_train.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cleaning Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are some columns that are not filled properly (with some empty rows), we decided to fill them in using linear interpolation. However, when we just use linear interpolation, the empty rows at the beginning cannot be filled since there is no number in front of the rows. Additionally, we add limit_direction='backward' to fully fill the last NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolate to fill in the data that is null\n",
    "prices_train = prices_train.interpolate(method='linear', limit_direction='backward')\n",
    "prices_train.isnull().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "## **EXPLORATORY DATA ANALYSIS**\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FINDING VALUE AT RISK VALUE**\n",
    "\n",
    "To get the value of VaR, we need some work first, namely\n",
    "1. Expected Values\n",
    "2. Mean of Expected Values\n",
    "3. Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Values\n",
    "The following is the expected values ​​search algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Value = Value(t) - Value(t-1) / Value(t-1)\n",
    "expected_df = (prices_train.diff() / prices_train.shift(1)).shift(-1)\n",
    "expected_df.columns = [f'{col}' for col in expected_df.columns]\n",
    "expected_df.dropna(how='all', inplace=True)\n",
    "expected_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Expected Value\n",
    "The following is a search for mean expected values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean (expected value) for each column in expected_df\n",
    "expected_means = expected_df.mean()\n",
    "expected_means.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "The following is a search for standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of daily returns for each stock\n",
    "std_deviation = expected_df.std()\n",
    "std_deviation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value at Risk\n",
    "Dengan memanfaatkan hasil dari pencarian di atas, kita mampu menemukan nilai value-at-risk sebagaimana ditunjukkan dibawah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Value at Risk\n",
    "value_at_risk = std_deviation.copy()\n",
    "value_at_risk = -(expected_means + std_deviation*norm.ppf(0.01))\n",
    "value_at_risk.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "### **CLUSTERING K-MEANS**\n",
    "\n",
    "After getting the VaR value, we will do clustering. There are several clustering methods used, namely,\n",
    "1. K-Means Algorithm\n",
    "2. Agglomerative Algorithm\n",
    "3. Gaussian Mixture Model (GMM) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it to a DataFrame with a single column\n",
    "Elbow = value_at_risk.to_frame()\n",
    "\n",
    "# Initialize an empty list to store the within-cluster sum of squares (WCSS) for different values of k\n",
    "wcss = []\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(1, 11)  # Testing k values from 1 to 10\n",
    "\n",
    "# Iterate over each value of k\n",
    "for k in k_values:\n",
    "    # Initialize KMeans with the current value of k\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit KMeans to the data\n",
    "    kmeans.fit(Elbow)\n",
    "    \n",
    "    # Append the WCSS (inertia_) to the list\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(k_values, wcss, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.xticks(np.arange(min(k_values), max(k_values)+1, 1.0))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **K-Means Algorithm**\n",
    "The following code is useful for performing clustering using the K-Means method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Clusters\n",
    "K = 5\n",
    "\n",
    "# Reshape the pandas Series into a 2D array with a single column\n",
    "X = value_at_risk.values.reshape(-1, 1)\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=K, random_state=42)\n",
    "clusters_kmeans = kmeans.fit_predict(X)\n",
    "clusters_kmeans = clusters_kmeans + 1\n",
    "\n",
    "# Display the resulting clusters\n",
    "result_df_kmeans = pd.DataFrame({'VaR': value_at_risk, 'Cluster': clusters_kmeans})\n",
    "print(result_df_kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pin Point Graph of Every Stocks Based On K-Means Algorithm\n",
    "The following code is useful for showing a stock distribution map based on Expected Values ​​and VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph for each cluster with a consistent color palette\n",
    "plt.figure(figsize=(8, 6))\n",
    "num_clusters = result_df_kmeans['Cluster'].nunique()\n",
    "color_palette = plt.cm.get_cmap('tab10', num_clusters) \n",
    "\n",
    "for i, cluster in enumerate(result_df_kmeans['Cluster'].unique()):\n",
    "    cluster_data_kmeans = result_df_kmeans[result_df_kmeans['Cluster'] == cluster]\n",
    "    plt.scatter(\n",
    "        cluster_data_kmeans['VaR'],\n",
    "        expected_means[cluster_data_kmeans.index],\n",
    "        color=color_palette(i),\n",
    "        label=f'Cluster {cluster}' \n",
    "    )\n",
    "\n",
    "plt.title('VaR vs. Expected Value by Cluster')\n",
    "plt.xlabel('Value at Risk (VaR)')\n",
    "plt.ylabel('Expected Value (Column Means)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Clustering Based On Selected Stocks on K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = \"BBCA\"\n",
    "cluster_series = result_df_kmeans.loc[stock_name, 'Cluster'] if stock_name in result_df_kmeans.index else None\n",
    "\n",
    "if cluster_series is not None:\n",
    "    cluster = cluster_series\n",
    "    print(f\"The stock {stock_name} is in Cluster {cluster}\\n\")\n",
    "    \n",
    "    # Find stocks that are in the same cluster as the selected one\n",
    "    same_cluster_stocks_kmeans = result_df_kmeans[result_df_kmeans['Cluster'] == cluster]\n",
    "\n",
    "    # Display stocks that are in the same cluster as the selected one\n",
    "    print(f\"Other stocks in Cluster {cluster}:\")\n",
    "    print(same_cluster_stocks_kmeans.head(5))\n",
    "else:\n",
    "    print(f\"The stock {stock_name} was not found in any cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CLUSTERING AGGLOMERATIVE ALGORITHM**\n",
    "Code di bawah berguna untuk melakukan clustering berdasarkan Algoritma Agglomerative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Clusters\n",
    "K = 5\n",
    "\n",
    "# Perform Agglomerative Clustering\n",
    "agg_cluster = AgglomerativeClustering(n_clusters=K)\n",
    "clusters_aglo = agg_cluster.fit_predict(value_at_risk.values.reshape(-1, 1))\n",
    "clusters_aglo = clusters_aglo + 1\n",
    "\n",
    "# Display the resulting clusters\n",
    "result_df_aglo = pd.DataFrame({'VaR': value_at_risk, 'Cluster': clusters_aglo})\n",
    "print(result_df_aglo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plot Every Stocks Based On Agglomerative Algorithm\n",
    "The code below is useful for visualizing distribution data based on Expected Values ​​and VaR from the Agglomerative algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick cluster count that is unique\n",
    "num_clusters_aglo = result_df_aglo['Cluster'].nunique()\n",
    "\n",
    "# Use the color palletes in order based on Matplotlib\n",
    "color_palette_aglo = plt.cm.tab10(np.linspace(0, 1, num_clusters_aglo))\n",
    "\n",
    "# Plot the graph for each cluster with a consistent color palette\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, cluster in enumerate(result_df_aglo['Cluster'].unique()):\n",
    "    cluster_data_aglo = result_df_aglo[result_df_aglo['Cluster'] == cluster]\n",
    "    plt.scatter(\n",
    "        cluster_data_aglo['VaR'],\n",
    "        expected_means[cluster_data_aglo.index],\n",
    "        color=color_palette_aglo[i], \n",
    "        label=f'Cluster {cluster}'\n",
    "    )\n",
    "\n",
    "plt.title('VaR vs. Expected Value by Cluster (Agglomerative)')\n",
    "plt.xlabel('Value at Risk (VaR)')\n",
    "plt.ylabel('Expected Value (Column Means)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Clustering Based On Selected Stocks on Agglomerative Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = \"BBCA\"\n",
    "cluster_series_aglo = result_df_aglo.loc[stock_name, 'Cluster'] if stock_name in result_df_aglo.index else None\n",
    "\n",
    "if cluster_series_aglo is not None:\n",
    "    cluster_aglo = cluster_series_aglo\n",
    "    print(f\"The stock {stock_name} is in Cluster {cluster_aglo}\\n\")\n",
    "    \n",
    "    # Find stocks that are in the same cluster as the selected one\n",
    "    same_cluster_stocks_aglo = result_df_aglo[result_df_aglo['Cluster'] == cluster_aglo]\n",
    "\n",
    "    # Display stocks that are in the same cluster as the selected one\n",
    "    print(f\"Other stocks in Cluster {cluster_aglo}:\")\n",
    "    print(same_cluster_stocks_aglo.head(5))\n",
    "else:\n",
    "    print(f\"The stock {stock_name} was not found in any cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GMM Algorithm**\n",
    "The code below is useful for implementing the GMM algorithm in clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Clusters\n",
    "K_gmm = 5\n",
    "\n",
    "# Perform GMM Clustering\n",
    "gmm_cluster = GaussianMixture(n_components=K_gmm, random_state=42)\n",
    "clusters_gmm = gmm_cluster.fit_predict(value_at_risk.values.reshape(-1, 1))\n",
    "clusters_gmm = clusters_gmm + 1\n",
    "\n",
    "# Display the resulting clusters\n",
    "result_df_gmm = pd.DataFrame({'VaR': value_at_risk, 'Cluster': clusters_gmm})\n",
    "print(result_df_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plot Every Stocks Based On GMM Algorithm\n",
    "The following is a visualization of all stocks based on Expected Values ​​and VaR from the GMM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for each cluster\n",
    "num_clusters_gmm = len(result_df_gmm['Cluster'].unique())\n",
    "color_palette_gmm = plt.cm.tab10(np.linspace(0, 1, num_clusters_gmm))\n",
    "\n",
    "# Plot the graph for each cluster with a consistent color palette\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, cluster in enumerate(result_df_gmm['Cluster'].unique()):\n",
    "    cluster_data_gmm = result_df_gmm[result_df_gmm['Cluster'] == cluster]\n",
    "    plt.scatter(\n",
    "        cluster_data_gmm['VaR'],\n",
    "        expected_means[cluster_data_gmm.index],\n",
    "        color=color_palette_gmm[i], \n",
    "        label=f'Cluster {cluster}'\n",
    "    )\n",
    "\n",
    "plt.title('VaR vs. Expected Value by Cluster (GMM)')\n",
    "plt.xlabel('Value at Risk (VaR)')\n",
    "plt.ylabel('Expected Value (Column Means)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Clustering Based On Selected Stocks on GMM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = \"BBCA\"\n",
    "cluster_series_gmm = result_df_gmm.loc[stock_name, 'Cluster'] if stock_name in result_df_gmm.index else None\n",
    "\n",
    "if cluster_series_gmm is not None:\n",
    "    cluster_gmm = cluster_series_gmm\n",
    "    print(f\"The stock {stock_name} is in Cluster {cluster_gmm}\\n\")\n",
    "    \n",
    "    # Find stocks that are in the same cluster as the selected one\n",
    "    same_cluster_stocks_gmm = result_df_gmm[result_df_gmm['Cluster'] == cluster_gmm]\n",
    "\n",
    "    # Display stocks that are in the same cluster as the selected one\n",
    "    print(f\"Other stocks in Cluster {cluster_gmm}:\")\n",
    "    print(same_cluster_stocks_gmm.head(5))\n",
    "else:\n",
    "    print(f\"The stock {stock_name} was not found in any cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "## COMPARISON\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean VaR of Clusters Analysis Based On Algorithm\n",
    "To find out the cluster similarities of each algorithm, it is necessary to carry out an analysis based on the mean VaR of each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Algorithm\n",
    "The following is a visualization of the Mean VaR K-Means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df_kmeans = result_df_kmeans.groupby('Cluster')['VaR'].mean().reset_index()\n",
    "cluster_labels = [f'Cluster {label}' for label in grouped_df_kmeans['Cluster']]\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(cluster_labels, grouped_df_kmeans['VaR'], color='skyblue')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Mean VaR')\n",
    "plt.title('Mean VaR by Cluster')\n",
    "plt.xticks(cluster_labels) \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Algorithm\n",
    "The following is a visualization of the Mean VaR Aggloemerative algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df_aglo = result_df_aglo.groupby('Cluster')['VaR'].mean().reset_index()\n",
    "cluster_labels_aglo = [f'Cluster {label}' for label in grouped_df_aglo['Cluster']]\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(cluster_labels_aglo, grouped_df_aglo['VaR'], color='skyblue')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Mean VaR')\n",
    "plt.title('Mean VaR by Cluster')\n",
    "plt.xticks(cluster_labels_aglo)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM Algorithm\n",
    "The following is a visualization of the Mean VaR GMM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df_gmm = result_df_gmm.groupby('Cluster')['VaR'].mean().reset_index()\n",
    "cluster_labels_gmm = [f'Cluster {label}' for label in grouped_df_gmm['Cluster']]\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(cluster_labels_gmm, grouped_df_gmm['VaR'], color='skyblue')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Mean VaR')\n",
    "plt.title('Mean VaR by Cluster')\n",
    "plt.xticks(cluster_labels_gmm) \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of 3 Algorithms\n",
    "The following is a comparison graph of Mean VaR for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.2\n",
    "\n",
    "# Cluster position on x-axis\n",
    "x_clusters = np.arange(len(grouped_df_kmeans))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar plot for K-means\n",
    "plt.bar(x_clusters - bar_width, grouped_df_kmeans['VaR'], width=bar_width, label='K-means')\n",
    "\n",
    "# Bar plot for Agglomerative clustering\n",
    "plt.bar(x_clusters, grouped_df_aglo['VaR'], width=bar_width, label='Agglomerative')\n",
    "\n",
    "# Bar plot ufor GMM\n",
    "plt.bar(x_clusters + bar_width, grouped_df_gmm['VaR'], width=bar_width, label='GMM')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Mean VaR')\n",
    "plt.title('VaR Values by Cluster for Different Methods')\n",
    "plt.xticks(x_clusters, [f'Cluster {label}' for label in grouped_df_gmm['Cluster']])\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display Plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "Labeling clusters (in the form of 1, 2, 3, 4, or 5) in the data grouping process using certain algorithms does not always follow the order from lowest to highest risk. The naming of clusters in the grouping method tends to be random. However, each cluster from different algorithms still has similar characteristics, even though the labels given may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Sorting Based On The Mean VaR\n",
    "In order to equate cluster names according to their characteristics, it is necessary to sort the clusters based on their Mean VaR value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_summary(df, algorithm_name):\n",
    "    # Extract numeric columns\n",
    "    numeric_columns = df.select_dtypes(include='number')\n",
    "    \n",
    "    # Calculate mean values for each cluster\n",
    "    cluster_means = numeric_columns.groupby('Cluster').mean()['VaR']\n",
    "    \n",
    "    # Explicitly sort the mean values\n",
    "    cluster_means_sorted = cluster_means.sort_values()\n",
    "    \n",
    "    # Create a DataFrame with mean values and cluster counts\n",
    "    summary = pd.DataFrame({\n",
    "        f'Mean VaR': cluster_means_sorted,\n",
    "        f'Counts': df['Cluster'].value_counts().reindex(cluster_means_sorted.index)\n",
    "    })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Get cluster summaries for each algorithm\n",
    "kmeans_summary = get_cluster_summary(result_df_kmeans, 'K-Means')\n",
    "aglo_summary = get_cluster_summary(result_df_aglo, 'Agglomerative')\n",
    "gmm_summary = get_cluster_summary(result_df_gmm, 'GMM')\n",
    "\n",
    "# Display the individual summaries\n",
    "print(\"K-Means Cluster Summary:\")\n",
    "print(kmeans_summary)\n",
    "\n",
    "print(\"\\nAgglomerative Cluster Summary:\")\n",
    "print(aglo_summary)\n",
    "\n",
    "print(\"\\nGMM Cluster Summary:\")\n",
    "print(gmm_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change of Cluster Order based On The Sorted Mean VaR\n",
    "With the data obtained, we will change the cluster name to adjust the three algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of DataFrame without changing the original one\n",
    "dfs = [result_df_kmeans, result_df_aglo, result_df_gmm]\n",
    "new_dfs = [df.copy() for df in dfs]\n",
    "\n",
    "# Determine the pattern of value subtitution\n",
    "clusters = [kmeans_summary, aglo_summary, gmm_summary]\n",
    "\n",
    "# Subtitute the value based on the pattern mentioned\n",
    "for i, df in enumerate(new_dfs):\n",
    "    df['Cluster'] = df['Cluster'].replace({clusters[i].index[j]: j+1 for j in range(5)})\n",
    "\n",
    "# Save the result on the variable created earlier\n",
    "newresult_df_kmeans, newresult_df_aglo, newresult_df_gmm = new_dfs\n",
    "\n",
    "# Plot mean VaR for each cluster and algorithm after reassigning clusters\n",
    "def get_cluster_summary(df, algorithm_name):\n",
    "    numeric_columns = df.select_dtypes(include='number')\n",
    "    cluster_means = numeric_columns.groupby('Cluster').mean()['VaR']\n",
    "    cluster_means_sorted = cluster_means.sort_values()\n",
    "    summary = pd.DataFrame({\n",
    "        f'Mean VaR': cluster_means_sorted,\n",
    "        f'Counts': df['Cluster'].value_counts().reindex(cluster_means_sorted.index)\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "# Get cluster summaries for each algorithm after reassigning clusters\n",
    "new_kmeans_summary = get_cluster_summary(newresult_df_kmeans, 'K-Means')\n",
    "new_aglo_summary = get_cluster_summary(newresult_df_aglo, 'Agglomerative')\n",
    "new_gmm_summary = get_cluster_summary(newresult_df_gmm, 'GMM')\n",
    "\n",
    "# Plot bar plot for mean VaR by cluster and algorithm after reassigning clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "bar_width = 0.2\n",
    "x_kmeans = np.arange(len(new_kmeans_summary))\n",
    "x_aglo = np.arange(len(new_aglo_summary)) + bar_width\n",
    "x_gmm = np.arange(len(new_gmm_summary)) + 2 * bar_width\n",
    "\n",
    "plt.bar(x_kmeans, new_kmeans_summary['Mean VaR'], width=bar_width, label='K-Means')\n",
    "plt.bar(x_aglo, new_aglo_summary['Mean VaR'], width=bar_width, label='Agglomerative')\n",
    "plt.bar(x_gmm, new_gmm_summary['Mean VaR'], width=bar_width, label='GMM')\n",
    "\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Mean VaR')\n",
    "plt.title('Mean VaR by Cluster')\n",
    "plt.xticks(np.arange(len(new_gmm_summary)) + bar_width, new_gmm_summary.index)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of The Number of Stocks in Every Cluster\n",
    "The following is an illustration of the number of stocks in each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of stocks in each cluster for each DataFrame\n",
    "kmeans_cluster_counts = newresult_df_kmeans['Cluster'].value_counts().sort_index()\n",
    "aglo_cluster_counts = newresult_df_aglo['Cluster'].value_counts().sort_index()\n",
    "gmm_cluster_counts = newresult_df_gmm['Cluster'].value_counts().sort_index()\n",
    "\n",
    "# Create a bar plot to compare the number of stocks in each cluster for each algorithm\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "bar_width = 0.2\n",
    "index = kmeans_cluster_counts.index  \n",
    "\n",
    "plt.bar(index - bar_width, kmeans_cluster_counts, width=bar_width, label='K-Means')\n",
    "plt.bar(index, aglo_cluster_counts, width=bar_width, label='Agglomerative')\n",
    "plt.bar(index + bar_width, gmm_cluster_counts, width=bar_width, label='GMM')\n",
    "\n",
    "# Display count values on top of each bar\n",
    "for i, v in enumerate(kmeans_cluster_counts):\n",
    "    plt.text(i + 1 - bar_width, v + 0.5, str(v), ha='center', va='bottom')\n",
    "for i, v in enumerate(aglo_cluster_counts):\n",
    "    plt.text(i+1, v + 0.5, str(v), ha='center', va='bottom')\n",
    "for i, v in enumerate(gmm_cluster_counts):\n",
    "    plt.text(i + 1 + bar_width, v + 0.5, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Stocks')\n",
    "plt.title('Number of Stocks in Each Cluster for Different Algorithms')\n",
    "plt.xticks(index)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Cluster 1\n",
    "The following is Stock data that is in cluster 1 in each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newresult_df_kmeans['Stocks'] = newresult_df_kmeans.index\n",
    "newresult_df_aglo['Stocks'] = newresult_df_aglo.index\n",
    "newresult_df_gmm['Stocks'] = newresult_df_gmm.index\n",
    "\n",
    "# Choose stocks with the cluster of 1 in each algorithm\n",
    "stocks_kmeans_cluster1 = newresult_df_kmeans[newresult_df_kmeans['Cluster'] == 1]['Stocks']\n",
    "stocks_aglo_cluster1 = newresult_df_aglo[newresult_df_aglo['Cluster'] == 1]['Stocks']\n",
    "stocks_gmm_cluster1 = newresult_df_gmm[newresult_df_gmm['Cluster'] == 1]['Stocks']\n",
    "\n",
    "# Find stocks that are in the cluster 1 on every algorithm\n",
    "stocks_in_clusters1 = set(stocks_kmeans_cluster1) & set(stocks_aglo_cluster1) & set(stocks_gmm_cluster1)\n",
    "\n",
    "#  Display and Check if the stocks are in the same cluster on every algorithm\n",
    "print(\"Stock berikut memiliki cluster 1 di ketiga algoritma:\")\n",
    "print(', '.join(f\"'{stock}'\" for stock in stocks_in_clusters1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Cluster 2\n",
    "The following is Stock data that is in cluster 2 in each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose stocks with the cluster of 2 in each algorithm\n",
    "stocks_kmeans_cluster2 = newresult_df_kmeans[newresult_df_kmeans['Cluster'] == 2]['Stocks']\n",
    "stocks_aglo_cluster2 = newresult_df_aglo[newresult_df_aglo['Cluster'] == 2]['Stocks']\n",
    "stocks_gmm_cluster2 = newresult_df_gmm[newresult_df_gmm['Cluster'] == 2]['Stocks']\n",
    "\n",
    "# Find stocks that are in the cluster 2 on every algorithm\n",
    "stocks_in_clusters2 = set(stocks_kmeans_cluster2) & set(stocks_aglo_cluster2) & set(stocks_gmm_cluster2)\n",
    "\n",
    "# Display and Check if the stocks are in the same cluster on every algorithm\n",
    "print(\"Stock berikut memiliki cluster 2 di ketiga algoritma:\")\n",
    "print(', '.join(f\"'{stock}'\" for stock in stocks_in_clusters2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Cluster 3\n",
    "The following is Stock data that is in cluster 3 in each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose stocks with the cluster of 3 in each algorithm\n",
    "stocks_kmeans_cluster3 = newresult_df_kmeans[newresult_df_kmeans['Cluster'] == 3]['Stocks']\n",
    "stocks_aglo_cluster3 = newresult_df_aglo[newresult_df_aglo['Cluster'] == 3]['Stocks']\n",
    "stocks_gmm_cluster3 = newresult_df_gmm[newresult_df_gmm['Cluster'] == 3]['Stocks']\n",
    "\n",
    "# Find stocks that are in the cluster 3 on every algorithm\n",
    "stocks_in_clusters3 = set(stocks_kmeans_cluster3) & set(stocks_aglo_cluster3) & set(stocks_gmm_cluster3)\n",
    "\n",
    "#  Display and Check if the stocks are in the same cluster on every algorithm\n",
    "print(\"Stock berikut memiliki cluster 3 di ketiga algoritma:\")\n",
    "print(', '.join(f\"'{stock}'\" for stock in stocks_in_clusters3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Cluster 4\n",
    "The following is Stock data that is in cluster 4 in each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose stocks with the cluster of 4 in each algorithm\n",
    "stocks_kmeans_cluster4 = newresult_df_kmeans[newresult_df_kmeans['Cluster'] == 4]['Stocks']\n",
    "stocks_aglo_cluster4 = newresult_df_aglo[newresult_df_aglo['Cluster'] == 4]['Stocks']\n",
    "stocks_gmm_cluster4 = newresult_df_gmm[newresult_df_gmm['Cluster'] == 4]['Stocks']\n",
    "\n",
    "# Find stocks that are in the cluster 4 on every algorithm\n",
    "stocks_in_clusters4 = set(stocks_kmeans_cluster4) & set(stocks_aglo_cluster4) & set(stocks_gmm_cluster4)\n",
    "\n",
    "#  Display and Check if the stocks are in the same cluster on every algorithm\n",
    "print(\"Stock berikut memiliki cluster 4 di ketiga algoritma:\")\n",
    "print(', '.join(f\"'{stock}'\" for stock in stocks_in_clusters4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Cluster 5\n",
    "The following is Stock data that is in cluster 5 in each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose stocks with the cluster of 5 in each algorithm\n",
    "stocks_kmeans_cluster5 = newresult_df_kmeans[newresult_df_kmeans['Cluster'] == 5]['Stocks']\n",
    "stocks_aglo_cluster5 = newresult_df_aglo[newresult_df_aglo['Cluster'] == 5]['Stocks']\n",
    "stocks_gmm_cluster5 = newresult_df_gmm[newresult_df_gmm['Cluster'] == 5]['Stocks']\n",
    "\n",
    "# Find stocks that are in the cluster 5 on every algorithm\n",
    "stocks_in_clusters5 = set(stocks_kmeans_cluster5) & set(stocks_aglo_cluster5) & set(stocks_gmm_cluster5)\n",
    "\n",
    "#  Display and Check if the stocks are in the same cluster on every algorithm\n",
    "print(\"Stock berikut memiliki cluster 5 di ketiga algoritma:\")\n",
    "print(', '.join(f\"'{stock}'\" for stock in stocks_in_clusters5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "## Features\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "### Note\n",
    "Value at Risk (VaR) is a method for estimating the potential loss of an investment with a certain level of confidence. For example, with a VaR of 0.56 at a 99% confidence level, there is a 99% probability that investment losses will not exceed 56% in a given period.\n",
    "\n",
    "In general, the lower the VaR value, the more stable and less volatile the stock price, indicating lower risk. Conversely, the higher the VaR value, the greater the share price fluctuations, indicating a higher level of risk in daily stock price changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging All The Clusters Into One Data Frame\n",
    "The code below is useful for unifying all clusters into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge three DataFrame based on 'Stock' and 'Cluster' Columns\n",
    "finalresult_df = pd.merge(newresult_df_kmeans, newresult_df_aglo, on=['VaR', 'Stocks', 'Cluster'])\n",
    "finalresult_df = pd.merge(finalresult_df, newresult_df_gmm, on=['VaR', 'Stocks', 'Cluster'])\n",
    "\n",
    "print(finalresult_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1\n",
    "#### Testing a specific Stock to where it falls in cluster while also recommending some stocks that fall in the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_info(cluster, cluster_data):\n",
    "    risk_levels = {\n",
    "        1: \"Sangat Rendah\",\n",
    "        2: \"Rendah\",\n",
    "        3: \"Sedang\",\n",
    "        4: \"Tinggi\",\n",
    "        5: \"Sangat Tinggi\"\n",
    "    }\n",
    "\n",
    "    print(f\"Cluster {cluster}: Investasi dalam klaster ini memiliki risiko yang {risk_levels.get(cluster, 'tidak dikenal')}.\\n\")\n",
    "\n",
    "    top_5_highest_var = cluster_data.head(5)\n",
    "    \n",
    "    print(f\"5 saham dengan Resiko Terendah di Cluster {cluster}:\")\n",
    "    print(top_5_highest_var[['VaR', 'Stocks', 'Cluster']], end='\\n\\n')\n",
    "\n",
    "stock_name = \"BBCA\"\n",
    "cluster_final = finalresult_df[finalresult_df.Stocks == stock_name]['Cluster']\n",
    "\n",
    "if not cluster_final.empty:\n",
    "    clusters_found = cluster_final.unique()\n",
    "    cluster_data_dict = {}\n",
    "\n",
    "    for cluster in clusters_found:\n",
    "        print(f\"Saham {stock_name} berada pada Cluster {cluster}\")\n",
    "        same_cluster_final = finalresult_df[finalresult_df['Cluster'] == cluster]\n",
    "        sorted_cluster = same_cluster_final.sort_values(by='VaR', ascending=True)\n",
    "        cluster_data_dict[cluster] = sorted_cluster.head(5)\n",
    "\n",
    "        print_cluster_info(cluster, cluster_data_dict[cluster])\n",
    "\n",
    "    additional_stocksfix = [cluster_data['Stocks'].tolist() for cluster_data in cluster_data_dict.values()]\n",
    "    stocks_to_plot = [item for sublist in additional_stocksfix for item in sublist] + [stock_name]\n",
    "\n",
    "    selected_stocks = prices_df[stocks_to_plot]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for stock in stocks_to_plot:\n",
    "        plt.semilogy(selected_stocks.index, selected_stocks[stock], label=stock)\n",
    "\n",
    "    plt.title('Grafik Harga Saham')\n",
    "    plt.xlabel('Tanggal')\n",
    "    plt.ylabel('Harga Saham')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"Saham {stock_name} tidak ditemukan di cluster manapun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 2\n",
    "#### Giving some stocks recommendation depending on the cluster risk level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih tingkat risiko\n",
    "tingkat_resiko = \"Tinggi\"\n",
    "\n",
    "# Dictionary for the mapping of the risk level\n",
    "tingkat_resiko_to_cluster = {\n",
    "    \"Sangat Rendah\": 1,\n",
    "    \"Rendah\": 2,\n",
    "    \"Sedang\": 3,\n",
    "    \"Tinggi\": 4,\n",
    "    \"Sangat Tinggi\": 5\n",
    "}\n",
    "\n",
    "# Check if the result is valid\n",
    "if tingkat_resiko in tingkat_resiko_to_cluster:\n",
    "    cluster = tingkat_resiko_to_cluster[tingkat_resiko]\n",
    "    emiten_cluster = finalresult_df[finalresult_df['Cluster'] == cluster]['Stocks']\n",
    "    \n",
    "    if not emiten_cluster.empty:\n",
    "        if len(emiten_cluster) < 10:\n",
    "            print(f\"Rekomendasi emiten untuk tingkat Risiko '{tingkat_resiko}':\")\n",
    "            for emiten in emiten_cluster:\n",
    "                print(emiten)\n",
    "        else:\n",
    "            emiten_rekomendasi = random.sample(emiten_cluster.tolist(), k=10)\n",
    "            print(f\"Rekomendasi 10 emiten untuk tingkat Risiko '{tingkat_resiko}':\")\n",
    "            for emiten in emiten_rekomendasi:\n",
    "                print(emiten)\n",
    "    else:\n",
    "        print(f\"Tidak ada emiten untuk tingkat Risiko '{tingkat_resiko}'\")\n",
    "else:\n",
    "    print(\"Tingkat risiko tidak valid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "## EVALUATION\n",
    "# ---------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the result of clustering from the all the algorithms\n",
    "test1_clusters_kmeans = result_df_kmeans['Cluster']  # Clustering result of K-Means\n",
    "test1_clusters_gmm = result_df_gmm['Cluster']   # Clustering result of GMM\n",
    "test1_clusters_aglo = result_df_aglo['Cluster']   # Clustering result of Agglomerical\n",
    "\n",
    "# Count the silhouette score of every clustering method\n",
    "silhouette_kmeans = silhouette_score(value_at_risk.values.reshape(-1, 1), test1_clusters_kmeans)\n",
    "silhouette_gmm = silhouette_score(value_at_risk.values.reshape(-1, 1), test1_clusters_gmm)\n",
    "silhouette_aglo = silhouette_score(value_at_risk.values.reshape(-1, 1), test1_clusters_aglo)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Silhouette Score K-Means: {silhouette_kmeans}\")\n",
    "print(f\"Silhouette Score GMM: {silhouette_gmm}\")\n",
    "print(f\"Silhouette Score Agglomerative Clustering: {silhouette_aglo}\")\n",
    "\n",
    "methods = ['K-Means', 'GMM', 'Agglomerative']\n",
    "scores = [silhouette_kmeans, silhouette_gmm, silhouette_aglo]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.barh(methods, scores)\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.title('Silhouette score for Different Clustering Methods')\n",
    "plt.xlim([0, max(scores) + 0.1])  # Adjust the x-axis limits if needed\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters=range(2,35,1)\n",
    "\n",
    "# Compute the silhouette score for K means cluster within the range\n",
    "scores = []\n",
    "for k in clusters:\n",
    "    km = KMeans(n_clusters=k,random_state=0, n_init=10)\n",
    "    labels = km.fit_predict(value_at_risk.values.reshape(-1, 1))\n",
    "    score = silhouette_score(value_at_risk.values.reshape(-1, 1),labels)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(clusters,scores)\n",
    "\n",
    "# Compute the silhouette score for Agglomerative cluster within the range\n",
    "scores = []\n",
    "for k in clusters:\n",
    "    ag = AgglomerativeClustering(n_clusters=k)\n",
    "    labels = ag.fit_predict(value_at_risk.values.reshape(-1, 1))\n",
    "    score = silhouette_score(value_at_risk.values.reshape(-1, 1),labels)\n",
    "    scores.append(score)\n",
    "\n",
    "plt.plot(clusters,scores)\n",
    "\n",
    "# Compute the silhouette score for GMM cluster within the range\n",
    "scores = []\n",
    "for k in clusters:\n",
    "    gm =GaussianMixture(n_components=k, random_state=42)\n",
    "    labels = gm.fit_predict(value_at_risk.values.reshape(-1, 1))\n",
    "    score = silhouette_score(value_at_risk.values.reshape(-1, 1),labels)\n",
    "    scores.append(score)\n",
    "\n",
    "plt.plot(clusters,scores)\n",
    "\n",
    "plt.plot(clusters,scores)\n",
    "plt.title('Silhouette Scores')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('silhouette score')\n",
    "plt.legend(['kmeans','Agglomerative','GMM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Davies-Bouldin Index for every method of clustering\n",
    "db_index_kmeans = davies_bouldin_score(value_at_risk.values.reshape(-1, 1), test1_clusters_kmeans)\n",
    "db_index_gmm = davies_bouldin_score(value_at_risk.values.reshape(-1, 1), test1_clusters_gmm)\n",
    "db_index_aglo = davies_bouldin_score(value_at_risk.values.reshape(-1, 1), test1_clusters_aglo)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Davies-Bouldin Index K-Means: {db_index_kmeans}\")\n",
    "print(f\"Davies-Bouldin Index GMM: {db_index_gmm}\")\n",
    "print(f\"Davies-Bouldin Index Agglomerative Clustering: {db_index_aglo}\")\n",
    "\n",
    "methods = ['K-Means', 'GMM', 'Agglomerative']\n",
    "scores = [db_index_kmeans, db_index_gmm, db_index_aglo]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.barh(methods, scores)\n",
    "plt.xlabel('Davies-Bouldin Index Score')\n",
    "plt.title('Davies-Bouldin Index for Different Clustering Methods')\n",
    "plt.xlim([0, max(scores) + 0.1])  # Adjust the x-axis limits if needed\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters=range(3,35,1)\n",
    "\n",
    "# Compute the Davies Bouldin score for K means cluster within the range \n",
    "scores_dbi = []\n",
    "for k in clusters:\n",
    "    km = KMeans(n_clusters=k,random_state=0, n_init=10)\n",
    "    labels = km.fit_predict(value_at_risk.values.reshape(-1, 1))\n",
    "    score = davies_bouldin_score(value_at_risk.values.reshape(-1, 1),labels)\n",
    "    scores_dbi.append(score)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(clusters,scores_dbi)\n",
    "\n",
    "# Compute the Davies Bouldin score for Agglomerative cluster within the range\n",
    "scores_dbi = []\n",
    "for k in clusters:\n",
    "    ag = AgglomerativeClustering(n_clusters=k)\n",
    "    labels = ag.fit_predict(value_at_risk.values.reshape(-1, 1))\n",
    "    score = davies_bouldin_score(value_at_risk.values.reshape(-1, 1),labels)\n",
    "    scores_dbi.append(score)\n",
    "\n",
    "plt.plot(clusters,scores_dbi)\n",
    "\n",
    "# Compute the Davies Bouldin score for GMM cluster within the range\n",
    "scores_dbi = []\n",
    "for k in clusters:\n",
    "    gm =GaussianMixture(n_components=k, random_state=42)\n",
    "    labels = gm.fit_predict(value_at_risk.values.reshape(-1, 1))\n",
    "    score = davies_bouldin_score(value_at_risk.values.reshape(-1, 1),labels)\n",
    "    scores_dbi.append(score)\n",
    "\n",
    "plt.plot(clusters,scores_dbi)\n",
    "\n",
    "plt.title('David-Bouldin Score')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('David-Bouldin Score')\n",
    "plt.legend(['kmeans','Agglomerative','GMM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Our results show that in our case with five clusters, Kmeans shows a better degree of similarity between the data in each cluster compared to other algorithms based on Silhouette calculations. On the other hand, when looking at the Davies-Bouldin Index (DBI), GMM shows a better level of member density in each cluster compared to the other two algorithms.\n",
    "\n",
    "In conclusion, from our graph analysis, Kmeans seems to be the best algorithm choice due to its better consistency within each cluster when evaluated with both Silhouette and DBI metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
